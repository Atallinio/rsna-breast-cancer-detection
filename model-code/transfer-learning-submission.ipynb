{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39272,"databundleVersionId":4629629,"sourceType":"competition"},{"sourceId":4913500,"sourceType":"datasetVersion","datasetId":2849545},{"sourceId":9612061,"sourceType":"datasetVersion","datasetId":5865245}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/joeatallah/rsna-bcd-tl-submission?scriptVersionId=200906892\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Imports and Installations","metadata":{}},{"cell_type":"code","source":"deps_path = '/kaggle/input/rsna-bcd-install-files'\n! pip install --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standard library imports\nimport os\nimport time\n\n# Third-party library imports\nimport cv2\nimport pydicom\nimport dicomsdl\n\n# Visualization library imports\nimport matplotlib.pyplot as plt\n\n# Progress bar library imports\nfrom tqdm.notebook import tqdm, trange\n\n# Parallel processing library imports\nfrom joblib import Parallel, delayed\n\nimport gc\n\nimport tensorflow as tf\nimport tensorflow_io as tfio\nimport tensorflow_datasets as tfds\nfrom tensorflow import keras\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport logging\nimport random\nimport pickle\nimport math\n\nfrom kaggle_datasets import KaggleDatasets\nfrom multiprocessing import cpu_count\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preporcessing\n\nMammographyPreprocessor class taken from [here](https://www.kaggle.com/code/paulbacher/custom-preprocessor-rsna-breast-cancer)","metadata":{}},{"cell_type":"code","source":"class MammographyPreprocessor():\n    \n    # Constructor\n    def __init__(self, size: tuple=None, breast_side: str='L',\n                 csv_path=None, train_path=None):\n        self.size = size\n        os.makedirs(os.getcwd(), exist_ok=True)\n        self.breast_side = breast_side\n        assert breast_side in ['L', 'R'], \"breast_side should be 'L' or 'R'\"\n        # implement the paths of the original RSNA dataset (V2)\n        self.csv_path = '/kaggle/input/rsna-breast-cancer-detection/train.csv'\n        self.train_path = '/kaggle/input/rsna-breast-cancer-detection/train_images'\n        if csv_path:\n            self.csv_path = csv_path\n        if train_path:\n            self.train_path = train_path\n        self.df = pd.read_csv(self.csv_path)\n        self.save_root = os.getcwd()\n    \n    # Get the paths from the preprocessor (V2)\n    def get_paths(self, n: int=None, shuffle: bool=False, return_cache: bool=False):\n        if n == None:\n            n = len(self.df)\n        if shuffle == True:\n            df = self.df.sample(frac=1, random_state=0).copy()\n        else:\n            df = self.df.copy()\n        paths = []\n        ids_cache = []\n        for i in range(n):\n            patient = str(df.iloc[i]['patient_id'])\n            scan = str(df.iloc[i]['image_id'])\n            paths.append(self.train_path + '/' + patient + '/' + scan + '.dcm')\n            ids_cache.append({'patient_id': patient, 'scan_id': scan})\n        if return_cache:\n            return paths, ids_cache\n        else:\n            return paths\n    \n    # Read from a path and convert to image array\n    def read_image(self, path: str):\n        scan = pydicom.dcmread(path)\n        img = scan.pixel_array\n        return img\n    \n    # Apply the preprocessing methods on one image\n    def preprocess_single_image(self, path: str, save: bool=False,\n                                save_dir: str=None, png: bool=True):\n        scan = dicomsdl.open(path)\n        img = scan.pixelData()\n        #img = self._windowing(img, scan)\n        img = self._fix_photometric_interpretation(img, scan)\n        img = self._normalize_to_255(img)\n        img = self._flip_breast_side(img)\n        img = self._crop(img)\n        if self.size:\n            img = self._resize(img)\n        if save:\n            self._save_image(img, path, png, save_dir)\n            return # do not return the images to avoid memory leak\n        return img\n    \n    # Preprocess all the images from the paths\n    def preprocess_all(self, paths: list, save: bool=True,\n                       save_dir: str='train_images', png: bool=True,\n                       parallel: bool=False, n_jobs: int=4):\n        clock = time.time()\n        if parallel:\n            Parallel(n_jobs=n_jobs) \\\n            (delayed(self.preprocess_single_image) \\\n            (path, save, save_dir, png) for path in tqdm(paths, total=len(paths)))\n            print(\"Parallel preprocessing done!\")\n        else:\n            for i in trange(len(paths)):\n                self.preprocess_single_image(paths[i], save, save_dir, png)\n            print(\"Sequential preprocessing done!\")\n        print(\"Time =\", np.around(time.time() - clock, 3), 'sec')\n    \n    # Adjust the contrast of an image\n    def _windowing(self, img, scan):\n        center = scan.WindowCenter\n        width = scan.WindowWidth\n        bits_stored = scan.BitsStored\n        function = scan.VOILUTFunction\n        if isinstance(center, list):\n            center = center[0]\n        if isinstance(width, list):\n            width = width[0] \n        y_range = float(2**bits_stored - 1)\n        if function == 'SIGMOID':\n            img = y_range / (1 + np.exp(-4 * (img - center) / width))\n        else: # LINEAR\n            center -= 0.5\n            width -= 1\n            below = img <= (center - width / 2)\n            above = img > (center + width / 2)\n            between = np.logical_and(~below, ~above)\n            img[below] = 0\n            img[above] = y_range\n            img[between] = ((img[between] - center) / width + 0.5) * y_range\n        return img\n    \n    # Interpret pixels in a consistant way\n    def _fix_photometric_interpretation(self, img, scan):\n        if scan.PhotometricInterpretation == 'MONOCHROME1':\n            return img.max() - img\n        elif scan.PhotometricInterpretation == 'MONOCHROME2':\n            return img - img.min()\n        else:\n            raise ValueError(\"Invalid Photometric Interpretation: {}\"\n                               .format(scan.PhotometricInterpretation))\n    \n    # Cast into 8-bits for saving\n    def _normalize_to_255(self, img):\n        if img.max() != 0:\n            img = img / img.max()\n        img *= 255\n        return img.astype(np.uint8)\n    \n    # Flip the breast horizontally on the chosen side \n    def _flip_breast_side(self, img):\n        img_breast_side = self._determine_breast_side(img)\n        if img_breast_side == self.breast_side:\n            return img\n        else:\n            return np.fliplr(img)    \n    \n    # Determine the current breast side\n    def _determine_breast_side(self, img):\n        col_sums_split = np.array_split(np.sum(img, axis=0), 2)\n        left_col_sum = np.sum(col_sums_split[0])\n        right_col_sum = np.sum(col_sums_split[1])\n        if left_col_sum > right_col_sum:\n            return 'L'\n        else:\n            return 'R'\n    \n    # Crop the useless background of the image\n    def _crop(self, img):\n        bin_img = self._binarize(img, threshold=5)\n        contour = self._extract_contour(bin_img)\n        img = self._erase_background(img, contour)\n        x1, x2 = np.min(contour[:, :, 0]), np.max(contour[:, :, 0])\n        y1, y2 = np.min(contour[:, :, 1]), np.max(contour[:, :, 1])\n        x1, x2 = int(0.99 * x1), int(1.01 * x2)\n        y1, y2 = int(0.99 * y1), int(1.01 * y2)\n        return img[y1:y2, x1:x2]\n    \n    # Binarize the image at the threshold\n    def _binarize(self, img, threshold):\n        return (img > threshold).astype(np.uint8)\n    \n    # Get contour points of the breast\n    def _extract_contour(self, bin_img):\n        contours, _ = cv2.findContours(\n            bin_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        contour = max(contours, key=cv2.contourArea)\n        return contour\n    \n    # Set to background pixels of the image to zero\n    def _erase_background(self, img, contour):\n        mask = np.zeros(img.shape, np.uint8)\n        cv2.drawContours(mask, [contour], -1, 255, cv2.FILLED)\n        output = cv2.bitwise_and(img, mask)\n        return output\n    \n    # Resize the image to the preprocessor size\n    def _resize(self, img):\n        return cv2.resize(img, self.size)\n    \n    # Get the save path of a given dicom file\n    def _get_save_path(self, path, png, save_dir):\n        patient = path.split('/')[-2]\n        filename = path.split('/')[-1]\n        if png:\n            filename = filename.replace('dcm', 'png')\n        else:\n            filename = filename.replace('dcm', 'jpeg')\n        if save_dir:\n            save_path = os.path.join(self.save_root, save_dir, patient, filename)\n        else:\n            save_path = os.path.join(self.save_root, patient, filename)\n        return save_path\n    \n    # Save the preprocessed image\n    def _save_image(self, img, path, png, save_dir):\n        save_path = self._get_save_path(path, png, save_dir)\n        patient_folder = os.path.split(save_path)[0]\n        os.makedirs(patient_folder, exist_ok=True)\n        cv2.imwrite(save_path, img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', TPU.master())\nexcept ValueError:\n    print('Running on CPU or GPU')\n    TPU = None\n\nif TPU:\n    IS_TPU = True\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    STRATEGY = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    IS_TPU = False\n    STRATEGY = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n#clear_output()\nN_REPLICAS = STRATEGY.num_replicas_in_sync\nprint(f'N_REPLICAS: {N_REPLICAS}, IS_TPU: {IS_TPU}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Image dimensions\nIMG_HEIGHT = 1456\nIMG_WIDTH = 728\nN_CHANNELS = 1\nINPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 1)\nN_SAMPLES_TFRECORDS = 548\n\n# Peak Learning Rate\nEPOCHS = 15\n\n# Batch size\nBATCH_SIZE = 8 * N_REPLICAS\n\nIS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\nVERBOSE = 1 if IS_INTERACTIVE else 2\n\n# Tensorflow AUTO flag\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(f'BATCH_SIZE: {BATCH_SIZE}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MIXED_PRECISION = False\nDEVICE = 'TPU'\n\nif MIXED_PRECISION:\n    if 'TPU' in DEVICE:\n        policy_type = 'mixed_bfloat16'\n    else:\n        policy_type = 'mixed_float16'\nelse:\n    policy_type = 'float32'\npolicy = tf.keras.mixed_precision.Policy(policy_type)\ntf.keras.mixed_precision.set_global_policy(policy)\nprint(f'Computation dtype: {tf.keras.mixed_precision.global_policy().compute_dtype}')\nprint(f'Variable dtype: {tf.keras.mixed_precision.global_policy().variable_dtype}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate the Test Images\n\npaths = sorted(tf.io.gfile.glob('/kaggle/input/rsna-breast-cancer-detection/test_images/*/*.dcm'))\n\ntest_dir = '/kaggle/working/test_images'\n\nmp = MammographyPreprocessor(size=(728, 1456))\nmp.preprocess_all(paths, save=True, save_dir=test_dir, parallel=True, n_jobs=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recursively get all PNG image paths from the directory and its subfolders\nimage_paths = sorted([os.path.join(root, fname) \n                      for root, _, files in os.walk(test_dir) \n                      for fname in files if fname.endswith('.png')])\n\n\n# Function to load and preprocess an image\ndef load_image(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_png(img, channels=1) \n    img = tf.image.resize(img, [1456, 728])\n    #img = img / 255\n    return img\n\n# Create a TensorFlow dataset from the image paths\ndataset = tf.data.Dataset.from_tensor_slices(image_paths)\n\n# Map the loading function to the dataset\ndataset = dataset.map(lambda x: load_image(x), num_parallel_calls=tf.data.AUTOTUNE)\n#plt.imshow(next(iter(dataset)).numpy())\n\n# Batch the dataset (set the batch size as needed)\ndataset = dataset.batch(8)\n\n# Prefetch for performance\ndataset = dataset.prefetch(tf.data.AUTOTUNE)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(next(iter(dataset))[0].numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\n@keras.saving.register_keras_serializable()\nclass pFBeta(tf.keras.Metric):\n    def __init__(self, beta=1, name='pF1', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.beta = beta\n        self.epsilon = 1e-10\n        self.pos = self.add_weight(name='pos', initializer='zeros')\n        self.ctp = self.add_weight(name='ctp', initializer='zeros')\n        self.cfp = self.add_weight(name='cfp', initializer='zeros')\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.keras.ops.cast(y_true, tf.float32)\n        y_pred = tf.keras.ops.clip(y_pred, 0, 1)\n        pos = tf.keras.ops.cast(tf.keras.ops.sum(y_true), tf.float32)\n        ctp = tf.keras.ops.cast(tf.keras.ops.sum(y_pred[y_true == 1]), tf.float32)\n        cfp = tf.keras.ops.cast(tf.keras.ops.sum(y_pred[y_true == 0]), tf.float32)\n        self.pos.assign_add(pos)\n        self.ctp.assign_add(ctp)\n        self.cfp.assign_add(cfp)\n\n    def result(self):\n        beta2 = self.beta * self.beta\n        prec = self.ctp / (self.ctp + self.cfp + self.epsilon)\n        reca = self.ctp / (self.pos + self.epsilon)\n        return (1 + beta2) * prec * reca / ((beta2 * prec + reca) + self.epsilon) \n    \n    def reset_state(self):\n        self.pos.assign(0.)\n        self.ctp.assign(0.)\n        self.cfp.assign(0.)\n    \n    def get_config(self):\n        return {\"beta\": self.beta, \"name\": self.name}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model(\"/kaggle/input/rsna_model/keras/default/1/rsna_model_v7.keras\")\nhistory = model.predict(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_csv = pd.read_csv(\"/kaggle/input/rsna-breast-cancer-detection/test.csv\")\ntest_csv[\"prediction_id\"] = test_csv[\"patient_id\"].astype(str) + \"_\" + test_csv[\"laterality\"].astype(str)\n\nsubmission = pd.DataFrame()\nsubmission[\"prediction_id\"] = test_csv['prediction_id']\nsubmission[\"cancer\"] = history[:,0]\nsubmission = submission.groupby('prediction_id').max()\nsubmission = submission.sort_index()\nsubmission.to_csv('submission.csv', index=True)\nsubmission.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}